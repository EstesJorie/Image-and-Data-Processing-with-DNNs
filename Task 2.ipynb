{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import glob\n",
    "import tarfile\n",
    "from PIL import Image, ImageDraw\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing and unzipping datasets for Google Colab integration\n",
    "\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "!unzip file.zip -d /content/my_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzImagesFilePath = 'images.tar.gz'\n",
    "gzAnnotationsFilePath = 'annotations.tar.gz'\n",
    "\n",
    "outputImagesFolder = 'oxfordPetImages/'\n",
    "outputAnnotationsFolder = 'oxfordPetAnnotations/'\n",
    "\n",
    "with tarfile.open(gzImagesFilePath, 'r:gz') as tar:\n",
    "    tar.extractall(path=outputImagesFolder)\n",
    "\n",
    "with tarfile.open(gzAnnotationsFilePath, 'r:gz') as tar:\n",
    "    tar.extractall(path=outputAnnotationsFolder)\n",
    "\n",
    "print(f\"Images extracted to {outputImagesFolder}\")\n",
    "print(f\"Images extracted to {outputAnnotationsFolder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petImagesFilePath = 'oxfordPetImages/images/' #path to images\n",
    "petAnnotations = 'oxfordPetAnnotations/annotations/trimaps' #path to trimaps\n",
    "petImageswithAnnotations = 'oxfordPetImages&Annotations/' #output for images with VGG16\n",
    "\n",
    "os.makedirs(petImageswithAnnotations, exist_ok=True) #ensures image output dir EXISTS\n",
    "features = []\n",
    "filenames = []\n",
    "groundTruthLabels =[]\n",
    "\n",
    "VGG = VGG16Classifier(num_classes=5)\n",
    "VGG.eval()\n",
    "\n",
    "checkpointFile = 'checkpoint.pkl'\n",
    "\n",
    "def saveProgress(features, filenames, groundTruthLabels):\n",
    "    with open(checkpointFile, 'wb') as f:\n",
    "        pickle.dump({'features': features, 'filenames': filenames, 'groundTruthLabels': groundTruthLabels}, f)\n",
    "    print(f\"Progress saved: {len(filenames)} images.\")\n",
    "\n",
    "def loadProgress():\n",
    "    if os.path.exists(checkpointFile):\n",
    "        with open(checkpointFile, 'rb') as f:\n",
    "            checkpoint = pickle.load(f)\n",
    "        return checkpoint['features'], checkpoint['filenames'], checkpoint['groundTruthLabels']\n",
    "    else:\n",
    "        return [], [], []\n",
    "\n",
    "features, filenames, groundTruthLabels = loadProgress()\n",
    "\n",
    "startIndex = len(filenames)\n",
    "\n",
    "transform = transform.Compose([ #initating transformations\n",
    "     transform.Resize((224,224)),\n",
    "     transform.ToTensor(),\n",
    "     transform.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "imagefilenames = [filename for filename in os.listdir(petImagesFilePath) if filename.endswith('.jpg') or filename.endswith('.png')]\n",
    "\n",
    "chunk_size = 100\n",
    "for i in tqdm(range(startIndex, len(imagefilenames), chunk_size), desc='Processing Chunks', unit='chunk'):\n",
    "    chunk = imagefilenames[i:i + chunk_size]\n",
    "    chunkFeatures = []\n",
    "    chunkFilenames = []\n",
    "    chunkLabels = []\n",
    "\n",
    "    with tqdm(chunk, desc=\"Processing images within chunk\", unit=\"image\", leave=False) as chunkBar:\n",
    "        for filename in chunkBar:\n",
    "                petImagePath = os.path.join(petImagesFilePath, filename)\n",
    "                try:\n",
    "                        img = Image.open(petImagePath).convert('RGB') #opens image in RGB mode\n",
    "                        imgResize = img.resize((224,224)) #resizes image for VGG16\n",
    "                        \n",
    "                        petImageTensor = transform(img).unsqueeze(0) #apply transformations to image\n",
    "                        with torch.no_grad(): #running image through VGG16 model\n",
    "                            feature = VGG(petImageTensor)\n",
    "                            feature = feature.view(-1).cpu().numpy()\n",
    "                            chunkFeatures.append(feature)\n",
    "                            chunkFilenames.append(filename)\n",
    "\n",
    "                        trimapFilename = filename.replace('.jpg', '.png').replace('.png', '.png')\n",
    "                        trimapPath = os.path.join(petAnnotations, trimapFilename)\n",
    "\n",
    "                        if os.path.exists(trimapPath):\n",
    "                            trimap = Image.open(trimapPath).convert('L')\n",
    "                            trimapResized = trimap.resize((224,224), Image.NEAREST) #resize to match image dimensions\n",
    "                            trimapArray = np.array(trimapResized)\n",
    "\n",
    "                            mostFreqLabel = np.bincount(trimapArray.flatten()).argmax()\n",
    "                            chunkLabels.append(trimapArray.flatten())\n",
    "                        else:\n",
    "                            print(f'Trimap for {filename} not found!')\n",
    "                except Exception as e:\n",
    "                        print(f'Error Processing {filename}: {e}')\n",
    "        \n",
    "        features.extend(chunkFeatures)\n",
    "        filenames.extend(chunkFilenames)\n",
    "        groundTruthLabels.extend(chunkLabels)\n",
    "\n",
    "        saveProgress(features, filenames, groundTruthLabels)\n",
    "\n",
    "features = np.array(features)\n",
    "groundTruthLabels = np.array(groundTruthLabels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224 #IMAGE SIZE\n",
    "\n",
    "source_dir = 'iROADSDataset'  # The original dataset directory\n",
    "train_dir = 'iRoads/train'\n",
    "validation_dir = 'iRoads/validation'\n",
    "\n",
    "\n",
    "# Create train and validation directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "def remove_macos_resource_forks(directory): #removes macOS resource forks\n",
    "    for filepath in glob.iglob(os.path.join(directory, '**', '._*'), recursive=True):\n",
    "        try:\n",
    "            os.remove(filepath)\n",
    "            print(f\"Removed: {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error removing file {filepath}: {e}\")\n",
    "\n",
    "directory = 'iRoads'  \n",
    "remove_macos_resource_forks(directory) #removes macOS resource forks (if present)\n",
    "categories = ['Daylight', 'Night', 'RainyDay', 'RainyNight', 'Snowy', 'SunStroke', 'Tunnel']\n",
    "\n",
    "#80% train, 20% validation\n",
    "split_ratio = 0.8\n",
    "for category in tqdm(categories, desc=\"Splitting Data\", unit=\"category\"):\n",
    "    source_class_dir = os.path.join(source_dir, category) #source for class\n",
    "    os.makedirs(os.path.join(train_dir, category), exist_ok=True)\n",
    "    os.makedirs(os.path.join(validation_dir, category), exist_ok=True)\n",
    "    files = [file for file in os.listdir(source_class_dir) if not file.startswith('._')]  #exclude system files like ._ files\n",
    "    random.shuffle(files)  # Shuffle the files to randomize split\n",
    "    \n",
    "    split_index = int(len(files) * split_ratio) #calc. split index\n",
    "    \n",
    "    for i, file in enumerate(tqdm(files, desc=f\"Processing {category}\", unit=\"file\", leave=False)):\n",
    "        source_file = os.path.join(source_class_dir, file)\n",
    "        \n",
    "        # If the index is less than the split index, move to train\n",
    "        if i < split_index:\n",
    "            shutil.move(source_file, os.path.join(train_dir, category, file))\n",
    "        else:\n",
    "            shutil.move(source_file, os.path.join(validation_dir, category, file))\n",
    "\n",
    "print(\"Data split complete.\")\n",
    "\n",
    "transform = transforms.Compose([ #transformation init defintion\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet mean and std\n",
    "])\n",
    "\n",
    "trainDataset = datasets.ImageFolder(train_dir, transform=transform) #loading train dataset\n",
    "validationDataset = datasets.ImageFolder(validation_dir, transform=transform) #load validation dataset\n",
    "\n",
    "trainLoader = DataLoader(train_dataset, batch_size=32, shuffle=True) # DATA load for both training and validation\n",
    "validationLoader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = models.resnet18(pretrained=True) #load ResNet18 model\n",
    "\n",
    "for param in model.parameters(): #freezing ResNet layers, minus FCL\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, len(categories))  #adjust ResNet FCL TO number of classes\n",
    "model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #entropy loss function\n",
    "\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.0001) #optimiser \n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, numEpochs=2):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(numEpochs), desc=\"Training Epochs\", unit=\"epoch\"):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in tqdm(trainLoader, desc=\"Training Batches\", unit=\"batch\", leave=False):\n",
    "            inputs, labels = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")), labels.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)  #cross entropy loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            runningLoss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) #output -> predicted\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {runningloss/len(trainLoader):.4f}, Accuracy: {100 * correct / total:.2f}%\") #stats of epoch\n",
    "\n",
    "def evaluateModel(model, validationLoader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        runningloss = 0.0\n",
    "        for inputs, labels in tqdm(validationLoader, desc=\"Evaluating Validation\", unit=\"batch\"):\n",
    "            inputs, labels = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")), labels.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            runningloss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) #output -> predicted\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy()) #store true labels for confusionMatrix\n",
    "            y_pred.extend(predicted.cpu().numpy()) #store predicted labels for confusionMatrix\n",
    "\n",
    "        print(f\"Validation Loss: {runningLoss/len(validationLoader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "trainModel(model, train_loader, criterion, optimizer, num_epochs=2) #model training\n",
    "\n",
    "y_true, y_pred = evaluateModel(model, validationLoader, criterion) #true and predicted labels from evaluated model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = np.sum(np.array(y_true) == np.array(y_pred)) / len(y_true) #accuracy metric\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted') #other performance metrics\n",
    "\n",
    "#print block for performance metrics\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "confusionMatrix = confusion_matrix(y_true, y_pred) #creation of confusionMatrix\n",
    "name = 'ResNet-18 iRoads' #name to link to output.png\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(style='whitegrid', palette='Blues')\n",
    "ax = sns.heatmap(confusionMatrix, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
    "                 xticklabels=categories, yticklabels=categories, linewidths=.8, linecolor='black')\n",
    "\n",
    "ax.set_xlabel(\"Predicted Labels\", fontsize=10)\n",
    "ax.set_ylabel(\"True Labels\", fontsize=10)\n",
    "ax.set_title(f\"Confusion Matrix for {name}\", fontsize=12, fontweight='bold')\n",
    "plt.savefig('confusionMatrix.png', bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "modelSavePath = 'resnet18_iroads_model.pth'\n",
    "torch.save(model.state_dict(), modelSavePath)\n",
    "print(f\"\\nModel saved at: {modelSavePath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(model, dataloader): #feature extraction for clustering\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataloader, desc=\"Extracting Features\", unit=\"batch\"):\n",
    "            inputs = inputs.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) #extract features from model before FCL\n",
    "            feature_vector = model.conv1(inputs)  #convolution layer\n",
    "            feature_vector = model.bn1(feature_vector)  #normalization\n",
    "            feature_vector = model.relu(feature_vector)  #ReLU activation\n",
    "            feature_vector = model.maxpool(feature_vector)  \n",
    "\n",
    "            feature_vector = model.layer1(feature_vector)\n",
    "            feature_vector = model.layer2(feature_vector)\n",
    "            feature_vector = model.layer3(feature_vector)\n",
    "            feature_vector = model.layer4(feature_vector)\n",
    "\n",
    "            feature_vector = feature_vector.mean(dim=[2, 3])  #Global average pooling\n",
    "            features.append(feature_vector.cpu().numpy())  \n",
    "            labels.extend(targets.cpu().numpy())  #Collect corresponding labels\n",
    "    return np.concatenate(features), np.array(labels)\n",
    "\n",
    "trainFeatures, trainLabels = extract_features(model, trainLoader) #feature extraction from training data\n",
    "\n",
    "kmeans = KMeans(n_clusters=len(categories), random_state=42) #k-means application to extracted features\n",
    "kmeans.fit(trainFeatures)\n",
    "\n",
    "DBindex = davies_bouldin_score(trainFeatures, kmeans.labels_) #DBI evalulation \n",
    "print(f\"Davies-Bouldin Index: {DBindex:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2) #PCA dimensionality reduction and visualisation\n",
    "reducedFeatures = pca.fit_transform(trainFeatures)\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = sns.scatterplot(x=reduced_features[:, 0], y=reduced_features[:, 1], \n",
    "                          hue=kmeans.labels_, palette=\"Set1\", s=50, edgecolor=\"black\")\n",
    "plt.title(\"K-Means Clustering (2D PCA Projection)\", fontsize=16, fontweight='bold')\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=10)\n",
    "plt.ylabel(\"Principal Component 2\", fontsize=10)\n",
    "plt.legend(title='Cluster', title_fontsize='11', fontsize='9', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('K-Means Clustering (2D PCA Projection).png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "def purity_score(y_true, y_pred): #clustering purity score\n",
    "    contingency_matrix = np.histogram2d(y_true, y_pred, bins=(len(np.unique(y_true)), len(np.unique(y_pred))))[0]\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "purity = purity_score(train_labels, kmeans.labels_)\n",
    "print(f\"Purity Score: {purity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
